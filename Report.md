# Multiagent DDPG Using PyTorch for the ML-agent Tennis Environment Report

### Introduction
This repository is an implementation of the Deep Deterministic Policy Gradient (DDPG) algorithm for the Tennis Environment developed by Unity3D and accessed through the UnityEnvironment library. It is an extension of the code sample provided by the Udacity Deep RL teaching crew (for more information visit their [website](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893)). The environment is presented as a vector; thus, we did not use Convolutional Neural Networks (CNN) \[[1](http://yann.lecun.com/exdb/publis/pdf/lecun-99.pdf)\] in the implementation.

The DQN algorithm and its derivatives were performing well in certain situations, but they were suffering from many shortcomings. The most important was their inability to tackle continuous action environments. The main reason for this is the curse of dimensionality present in these environments, which makes them slow even after discretization. Thus, the Deep Deterministic Policy Gradient (DDPG) was proposed in 2016 \[[2](https://arxiv.org/abs/1509.02971)\], which was an enhanced version of the original DPG paper \[[3](http://proceedings.mlr.press/v32/silver14.pdf)\]. DDPG is inherently model-free and actor-critic \[[2](https://arxiv.org/abs/1509.02971)\].

DDPG borrowed the main algorithm from the DPG paper, but in architecture and learning mechanism, it is following the footsteps of the DQN algorithm \[[4](https://arxiv.org/abs/1312.5602)\], namely using experience replay and the target network. The actor-critic nature of DDPG requires the initialization of two interacting neural networks. In each iteration, the actor-network generates an action according to the current policy and Ornstein-Uhlenbeck random process as its exploration noise. After performing the chosen action, observing the new state and receiving the reward, the critic network tries to minimize the loss function, which also influences the actor policy. The final step is to update the target network using the <img src="https://render.githubusercontent.com/render/math?math=\tau"> hyperparameter, which controls the amount of update. Instead of the delayed update method in DQN, DDPG instantly updates the target function using <img src="https://render.githubusercontent.com/render/math?math=\tau">. The DDPG pseudocode is available in Figure 1.

For consistancy, DDPG follows DQN's architecture which had five-layer neural network architecture for the pixel-based training. The input to the network is the environment state, and its output is the set of Q-values for the critic and the action policy for the actor. When a vectorized environment is used, the CNN part can be cautiously removed, utilizing the provided vectors as features generated by the CNN. DDPG success can be attributed to two main innovative techniques:
1. Actor-Critic Paradigm
2. Deterministic Policy
3. Efficient way of using Neural Nets (and Target Networks)
4. Experience Replay

### Method
DDPG, as represented in Figure 1, executes a typical reinforcement learning algorithm. It gathers a repository of experiences or transitions while exploring the environment. This dataset is collected by a behavior policy which is being updated more regularly. The target policy, which determines the final policy of the agent, is updated on a slower rate. The critic uses a TD style loss function, whereas the actor utilizes an objective derived from the Policy Gradient method.

<p align="center">
    <img src="https://github.com/FredAmouzgar/DDPG_PyTorch/raw/master/images/ddpg.png" width="600" height="500"><br>
    <p align="center" style="font-size=70%;">Figure 1: The DDPG<a href="https://arxiv.org/abs/1509.02971">[2]</a></p>
</p>

### The Tennis Environment
In this environment, two agents control rackets to bounce a ball over a net. If an agent hits the ball over the net, it receives a reward of +0.1. If an agent lets a ball hit the ground or hits the ball out of bounds, it receives a reward of -0.01. Thus, the goal of each agent is to keep the ball in play

<p align="center">
    <img src="https://github.com/FredAmouzgar/Multiagent_DDPG_PyTorch_Tennis/raw/master/images/Tennis.jpeg" width="400" height="200">
    <p align="center" style="font-size=70%;">Figure 2: The Tennis Environment</p>
</p>

The observation space consists of 8 variables corresponding to the position and velocity of the ball and racket. Each agent receives its own, local observation. Two continuous actions are available, corresponding to movement toward (or away from) the net, and jumping.

The Tennis environment consists of two unity agents. Each agent collects obseravations from itself and its opponents which make the entire activity a collaborative problem. Thus, they learn together how to maximize the reward.

### Experiments
This repository consists of these files:

*These files are saved under the "src" directory.*
1. <ins> model.py </ins>: This module provides the underlying neural network for our agent. When we train our agent, this neural network is going to be updated by backpropagation.
2. <ins>replay_buffer.py</ins>: This module implements the "memory" of our agent, also known as the Experience Replay.
3. <ins>agent.py</ins>: This is the body of our agent. It implements the way the agent acts using an actor-critic paradigm, and learn an optimal policy.
4. <ins>train.py</ins>: This module has the train function which takes the agent, the environment, number of training episodes and the required hyper-parameters and trains the agent accordingly.
*These files are saved under the "src" directory.*

To test the code, after cloning the project, open the `Tennis.ipynb` notebook. It has all the necessary steps to install and load the packages, and train and test the agent. It also automatically detects the operating system, and loads the corresponding environment. There are two pairs of already trained agents stored in `checkpoint-actor.pth` and `checkpoint-critic.pth`, by running the last part of the notebook, their performance can be directly tested.

Figure 3 is depicted a reward plot acquired by the agent while learning. It surpasses +33 after around 125 episodes.

<p align="center">
    <img src="" width="400" height="200"><br><p align="center" style="font-size=70%;">Figure 3: The average reward during training</p>
</p>

Figure 4 shows one episode after training.

<p align="center">
    <img src="https://github.com/FredAmouzgar/Multiagent_DDPG_PyTorch_Tennis/raw/master/images/Tennis.gif" width="400" height="200"><br>
    <p align="center" style="font-size=70%;">Figure 4: A Trained Agent</p>
</p>

### Hyperparameters
__Reacher Environment:__
- State size: 48
- Action size: 2

__DDPG Agent:__
- Replay memory buffer size: <img src="https://render.githubusercontent.com/render/math?math=10 ^ 5">
- Batch size: 128
- <img src="https://render.githubusercontent.com/render/math?math=\gamma">: 0.99
- <img src="https://render.githubusercontent.com/render/math?math=\tau"> for target update: <img src="https://render.githubusercontent.com/render/math?math=5\times 10^{-3}">
- Actor learning rate: <img src="https://render.githubusercontent.com/render/math?math=10 ^ {-3}">
- Critic learning rate: <img src="https://render.githubusercontent.com/render/math?math=10 ^ {-3}">

__Neural Networks:__
- Input Layer: 48 (state size)
- Hidden Layer 1: 256 units
- Hidden Layer 2: 128 units
- Output Layer: 2 (action size)
- Activation function (output layer): 
    1. Actor: Tanh
    2. Critic: linear

__Training Parameters:__
- Episodes: 125
- Steps in every episode: Depends on the agents' success

### Future Work:
1. Trying other hyperparameters
2. Implementing MADDPG
3. Incorporating and testing the multiagent style with other state-of-the-art algorithms such as PPO, A2C, and SAC.