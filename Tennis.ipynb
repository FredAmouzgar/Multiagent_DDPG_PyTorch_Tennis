{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tennis (Multi-agent Problem)¶\n",
    "\n",
    "Use this notebook to train and/or test your agent for the Tennis environment. Follow the instructions below to get started!\n",
    "\n",
    "## 1. Start the Environment\n",
    "Run the next code cell to install a few packages. This line will take a few minutes to run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install ./python tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environments corresponding to both versions of the environment are already saved in the Workspace and can be accessed at the file paths provided below.\n",
    "\n",
    "Please select one of the two options below for loading the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading the Environment for Training¶\n",
    "It automatically detects the platform and assigns the right file to the UnityEnvironment. The assumption is that the computer is 64 bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "import platform\n",
    "\n",
    "platform_files = {\"Windows\":r\".\\ENVs\\Tennis_Windows_x86_64\\Tennis.exe\",\n",
    "                 \"Linux\":r\"./ENVs/Tennis_Linux/Tennis.x86_64\",\n",
    "                 \"Darwin\":r\"./ENVs/Tennis.app\"}\n",
    "file = platform_files[platform.system()]\n",
    "env = UnityEnvironment(file_name=file , no_graphics=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initializing our agent with environment parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.agent import Agent\n",
    "from collections import deque\n",
    "\n",
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of actions\n",
    "action_size = brain.vector_action_space_size\n",
    "\n",
    "# Init the state space and finding its size\n",
    "state = env_info.vector_observations[0]\n",
    "state_size = len(state)\n",
    "num_agents = len(env_info.agents)\n",
    "\n",
    "# Here we initialize two agents\n",
    "# We set the states size to 48 (24*2), so we can feed each agent boths agent's state observations.\n",
    "agent_1 = Agent(state_size=48, action_size=action_size, num_agents=1, random_seed=0)\n",
    "agent_2 = Agent(state_size=48, action_size=action_size, num_agents=1, random_seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training our Tennis Agents for 5000 Episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 4402 | Avg Score: 1.99:  88%|████████████████████████████████████▉     | 4403/5000 [4:25:33<4:50:20, 29.18s/it]"
     ]
    }
   ],
   "source": [
    "from src.train import train_multiagent\n",
    "episode_scores, average_scores = train_multiagent(agent_1, agent_2, env, num_agents, n_episodes=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation: The agent was trained for 5000 episodes. It surpasses the average reward of +0.5 at around episode 4000, and it continues to grow after that till it get to +2 after "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Plotting the Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the scores\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(episode_scores)), episode_scores)\n",
    "plt.plot(np.arange(len(average_scores)), average_scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.legend([\"Episode Scores\", \"Average Scores\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Watch a smart Agent\n",
    "If you trained the agent or you just want to see a trained agent behavior, don't forget to __restart the notebook__ and run the section below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n",
      "                                                                                                                       \r"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "import platform\n",
    "from src.agent import Agent\n",
    "from collections import deque\n",
    "from src.train import train_multiagent\n",
    "\n",
    "platform_files = {\"Windows\":r\".\\ENVs\\Tennis_Windows_x86_64\\Tennis.exe\",\n",
    "                 \"Linux\":r\"./ENVs/Tennis_Linux/Tennis.x86_64\",\n",
    "                 \"Darwin\":r\"./ENVs/Tennis.app\"}\n",
    "file = platform_files[platform.system()]\n",
    "env = UnityEnvironment(file_name=file , no_graphics=False)\n",
    "\n",
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "# reset the environment\n",
    "env_info = env.reset(train_mode=False)[brain_name]\n",
    "\n",
    "# number of actions\n",
    "action_size = brain.vector_action_space_size\n",
    "\n",
    "# Init the state space and finding its size\n",
    "state = env_info.vector_observations[0]\n",
    "state_size = len(state)\n",
    "num_agents = len(env_info.agents)\n",
    "\n",
    "# Here we initialize two agents\n",
    "# We set the states size to 48 (24*2), so we can feed each agent boths agent's state observations.\n",
    "agent_1 = Agent(state_size=48, action_size=action_size, num_agents=1, random_seed=0)\n",
    "agent_2 = Agent(state_size=48, action_size=action_size, num_agents=1, random_seed=0)\n",
    "agent_1.load_brain(agent_prefix=\"ag_1\")\n",
    "agent_2.load_brain(agent_prefix=\"ag_2\")\n",
    "\n",
    "# Tesing for 2 episodes. Notice that I set the train_mode parameter to False\n",
    "# which deactivates the train_mode in ML Agents and prevents learning.\n",
    "episode_scores, average_scores = train_multiagent(agent_1, agent_2, env, num_agents, n_episodes=2, train_mode=False)\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
